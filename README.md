# Optical Music Recognition (OMR) Pipeline

## Project Overview

This project implements an Optical Music Recognition pipeline designed to transcribe handwritten sheet music into ABC notation. It uses Vision Language Models (VLMs) such as **Qwen2-VL** and **SmolVLM** to perform recognition. The system is right now optimized for local execution on **macOS (Apple Silicon/MPS)**.

The pipeline manages the entire lifecycle: environment setup, model caching, inference, resource monitoring, and evaluation against ground truth data.

## Project Structure

### Source & Configuration

```text
├── .env                              # Environment variables (HF Token)
├── main.py                           # Main Python entry point
├── requirements.txt                  # Python dependencies
├── config/
│   ├── main_config.yaml              # Active experiment selection
│   ├── models_config.yaml            # Model parameters & HuggingFace IDs
│   ├── paths_config.yaml             # Directory paths configuration
│   ├── plans_config.yaml             # Experiment plans definitions
│   └── prompts.yaml                  # System prompts for VLM
├── data/
|   ├── 1/
|   │   ├── 1.png                     # Input image
|   │   └── 1.txt                     # Ground truth (ABC Notation)
|   ├── 2/
|   │   ├── 2.png
|   │   ├── 2.txt
|   |   └── ...
|   └── ...
├── scripts/
│   └── run.sh                        # Bash entry point (setup + run)
└── src/
    ├── __init__.py
    ├── data_loader.py                # Data loading logic
    ├── evaluation.py                 # Levenshtein & SER metrics
    ├── loggers.py                    # Logging configuration
    ├── monitor.py                    # Hardware usage & TPS monitoring
    └── pipeline/
        ├── __init__.py
        ├── model_client.py           # VLM wrappers (Qwen/SmolVLM)
        └── runner.py                 # Main inference loop
```

### Output Files & Artifacts

After execution, the following files are generated:

```text
├── logs/
│   ├── run_YYYY_MM_DD_HH_MM_SS.txt   # Execution logs
│   ├── cpu_usage.png                 # Hardware monitoring plots
│   ├── gpu_memory.png
│   ├── latency.png
│   ├── ram_usage.png
│   └── throughput.png
├── .cache/                           # Model weights cache
├── omr_venv/                         # Virtual environment
└── data/
    └── {ID}/
        └── {ID}_det.txt              # Generated detection file
```

## Setup Instructions

### 1. Hugging Face Token

1.  Obtain a User Access Token with `read` permissions from Hugging Face Settings.
2.  Create a file named `.env` in the root directory.
3.  Add the token to the file:

```bash
HF_TOKEN=hf_YourTokenHere
```

### 2. Data Preparation

Organize your dataset in the `data/` directory. Each image must have its own subdirectory named after the image ID.

**Structure:**

```text
data/
├── 1/
│   ├── 1.png      # Input image
│   ├── 1.txt      # Ground truth (ABC Notation)
│   └── 1_det.txt  # Output (Generated by system)
├── 2/
│   ├── 2.png
│   ├── 2.txt
│   └── ...
```

You can add your own custom images by creating new folders (e.g., `3/`) containing the image file and the ground truth text file.

### 3. ABC Notation Overview

The system is prompted to output strict ABC notation.

*   **Header:**
    *   `M:4/4` - Meter / Time Signature.
    *   `L:1/4` - Default Note Length.
    *   `K:C` - Key Signature (e.g., C Major).
*   **Notes:**
    *   Pitch is denoted by letters: `C`, `D`, `E`, `F`, `G`, `A`, `B`.
    *   Duration is denoted by numbers: `E2` (Half note), `E` (Quarter note), `E/2` (Eighth note).
*   **Structure:**
    *   `|` - Bar line separator.
    *   `z` - Rest.

## Execution

To run the pipeline, ensure the script is executable and then run it from the project root:

```bash
chmod +x ./scripts/run.sh
./scripts/run.sh
```

**Process Flow:**

1.  **Environment Setup:** Checks for `omr_venv`. Creates it and installs dependencies from `requirements.txt` if missing.
2.  **Activation:** Activates the virtual environment.
3.  **Authentication:** Logs into Hugging Face using the token from `.env`.
4.  **Caching:** Model weights are downloaded to `.cache/huggingface`. This happens only on the first run. Subsequent runs use the cached files.
5.  **Inference:** Executes `main.py`, which triggers the Python pipeline.

## Configuration Details

*   **`config/main_config.yaml`**:
    *   `active_experiment_plan`: Selects which plan from `plans_config.yaml` to execute.

*   **`config/plans_config.yaml`**:
    *   Defines experiment scenarios.
    *   Links a `model_key` (architecture) with a `prompt_key` (instruction).
    *   `data_limit`: Controls how many images to process (-1 for all).

*   **`config/models_config.yaml`**:
    *   Defines model specifications.
    *   `model_id`: Hugging Face repository ID.
    *   `model_kwargs`: Loading parameters (e.g., `dtype: "bfloat16"`).
    *   `generate_kwargs`: Inference parameters (see below).

*   **`config/prompts.yaml`**:
    *   Contains system instructions passed to the LLM.
    *   Defines formatting rules for ABC notation output.

## Adding New Models

To integrate a new Hugging Face model:

1.  **Update `models_config.yaml`**: Add a new entry with the model's ID and parameters.
    ```yaml
    new_model_key:
      model_id: "Organization/Model-Name"
      model_kwargs:
        dtype: "bfloat16"
    ```

2.  **Update `plans_config.yaml`**: Create a plan that uses `new_model_key`.

3.  **Update `main_config.yaml`**: Set `active_experiment_plan` to your new plan.

4.  **Update `src/pipeline/model_client.py`**: If the model requires a specific chat template handling or processor logic, add a dedicated client class (ModelClient...) and update the factory function `get_model_client`.

## Model Parameters

The `models_config.yaml` file controls inference behavior:

*   `dtype`: Precision format. `bfloat16` is recommended.
*   `low_cpu_mem_usage`: Optimizes memory loading sequence.
*   `max_new_tokens`: Maximum length of the generated text.
*   `do_sample`: If `false`, uses greedy decoding (deterministic). If `true`, introduces randomness.
*   `temperature`: Controls randomness when sampling is enabled. Lower values (e.g., 0.1) make output more focused.
*   `repetition_penalty`: Penalizes repetitive tokens to prevent loops.

## Limitations

*   Currently optimized for **macOS (MPS)** devices.
*   Supports detection of simple melodic lines.
*   Complex polyphony and dense chords may not be recognized accurately due to the use of relatively small local models (2B parameters).

## TODO

*   Implement for CUDA, currently supported for MPS on MacOS M1.
*   Test bigger models.
*   Test different model parameters (temperature, top_p, repetition_penalty).
*   Add more context for bigger models (maybe even with image examples in prompt).
*   Make own handwritten sheet music in good quality and readable to test (potentially consider decreasing resolution to avoid significantly increasing model context).
